# AI Security Newsletter: February 3, 2026

## Section 1: Core AI Security

### Top 10 Security Highlights
1. **Exposed LLM Services Under Siege**: Two massive campaigns have targeted over 90,000 public LLM endpoints, focusing on secret extraction and mapping enterprise attack surfaces. [Dark Reading](https://www.darkreading.com/endpoint-security/separate-campaigns-target-exposed-llm-services)
2. **"Jailbreaking to Jailbreak"**: Researchers from Scale Red Team and others released a framework demonstrating how LLMs can be utilized to automatedly red team other models effectively. [arXiv:2502.09638](https://arxiv.org/abs/2502.09638)
3. **OpenRT Framework Launched**: OpenRT, a modular, high-throughput red-teaming framework for Multimodal LLMs (MLLMs), has been released to address fragmented safety benchmarks. [arXiv:2601.01592](https://arxiv.org/abs/2601.01592)
4. **LLM Agent Vulnerabilities**: A new study shows commercial LLM agents are still susceptible to "simple yet dangerous" attacks involving tool-use manipulation. [arXiv:2502.08586](https://arxiv.org/abs/2502.08586)
5. **OpenAI Red Teaming Methodology**: OpenAI published their current approach to external red teaming, emphasizing the critical role of independent auditors in model safety. [arXiv:2503.16431](https://arxiv.org/abs/2503.16431)
6. **AI Bot Swarms Alert**: Security experts, including Gary Marcus, warn that coordinated AI bot swarms are becoming sophisticated enough to fake majorities and undermine democratic discourse. [Science/Gary Marcus](https://open.substack.com/pub/garymarcus/p/ai-bot-swarms-threaten-to-undermine)
7. **Personal Intelligence Risks**: Google’s new "Personal Intelligence" mode raises significant privacy concerns by deep-linking across ecosystem search/history breadcrumbs. [MarketingProfs](https://www.marketingprofs.com/opinions/2026/54252/ai-update-january-30-2026-ai-news-and-views-from-the-past-week)
8. **Alignment Pretraining Feedback Loop**: New research explores how AI discourse in pretraining corpora causes "self-fulfilling" alignment or misalignment in future models. [arXiv:2601.10160](https://arxiv.org/abs/2601.10160)
9. **Autonomous Pentesting Stream**: "Off By One Security" hosted an in-depth session on the current 2026 state of autonomous penetration testing against web apps and LLM chatbots. [YouTube](https://www.youtube.com/watch?v=SHJeeP1gaBo)
10. **The "Phoenix" Algorithm**: X (Twitter) has integrated "Phoenix" (based on Grok-1) into its ranking pipeline, marking a major shift in how AI-driven engagement scores are calculated. [Alpha Stack](https://www.youtube.com/watch?v=6LJAyOSsbQA)

### Deep Dives
*   **The Rise of AI Bot Swarms**: Traditional botnets were "megaphones," but the new generation of swarms acts as a "coordinated social organism." These swarms can maintain consistent personas across platforms, making them nearly indistinguishable from human movements in democratic processes.
*   **Multimodal Red Teaming (OpenRT)**: As we move from text to multimodal (image/video/audio) assistants, safety benchmarks are lagging. OpenRT provides an adversarial kernel that separates the attack logic from the modality, allowing for much faster security iteration on models like GPT-5 and Gemini 2.0.

### Watch List
*   **Tool-Use Integrity**: Watch for "MitM" (Man-in-the-Middle) style proxies that intercept LLM tool calls to monitor sensitive data leaks.
*   **Grok-based Ranking**: Monitor how the open-sourcing of X’s "Phoenix" model affects information integrity and bot-detection heuristics.

### Security Recommendations
1.  **Audit Public LLM Endpoints**: Immediately scan for exposed internal LLM API keys or endpoints that do not have strict rate-limiting and secret-sanitization.
2.  **Multimodal Safety Guards**: If deploying multimodal models, implement modality-specific filters (e.g., image-to-text safety checks) before the primary agent processes the input.

---

## Section 2: General Tech & Daily Feed

### GitHub Trending Repos
*   [kortix-ai / suna](https://github.com/kortix-ai/suna): Open Source Generalist AI Agent.
*   [nari-labs / dia](https://github.com/nari-labs/dia): Ultra-realistic dialogue Generation (TTS).
*   [appcypher / ai-trending](https://github.com/appcypher/ai-trending): Real-time tracking of AI repositories.
*   [x1xhlol / v0-system-prompts](https://github.com/x1xhlol/v0-system-prompts): Full collection of system prompts for AI coding agents.

### Daily Motivation Quote
> "Everything you can imagine is real." — *Pablo Picasso*

### Top Hacker News Stories
*   [Somebody used spoofed ADSB signals to raster the meme of JD Vance](https://hn.news/)
*   [Trinity large: An open 400B sparse MoE model](https://hn.news/)
*   [Show HN: A MitM proxy to see what your LLM tools are sending](https://news.ycombinator.com/item?id=60)
*   [Building my self-hosted cloud coding agent](https://stanislas.blog)

### AI Twitter High-Signal Highlights
*   **X Algorithm Weights**: The 2026 X algorithm weights reveal a bookmark is now worth 10x a like, and author-replied threads are worth 150x. [Typefully](https://typefully.com/blog/x-algorithm-open-source)
*   **Codeless Ecosystem**: Anil Dash highlights a "remarkable leap" in orchestrating coding bots, enabling "codeless" software production by ordinary creators. [AnilDash.com](https://anildash.com/2026/01/27/codeless-ecosystem/)

# AI Security Brief — 2026-02-01

> **Status:** *no-live-sources* (web search not configured on this host yet).  
> This is a best-effort, practitioner-focused brief based on general knowledge + common failure modes.  
> **TODO:** add citations once web search is enabled.

## 10 things to know (practitioner bullets)
1. **Prompt injection remains the #1 practical risk** for LLM apps that ingest untrusted content (tickets, emails, docs, web pages). Treat untrusted text as hostile.
2. **Tool/agent permissions are your blast radius.** The most common “AI incident” pattern is over-privileged tool access + weak scoping.
3. **Retrieval ≠ trust.** RAG increases the chance of plausible-sounding errors if you don’t enforce source-of-truth constraints and output provenance.
4. **Model context is a data leak boundary.** Any secret placed in context is one jailbreak away from being exfiltrated unless you hard-segment.
5. **Logging can become a shadow data lake** (PII, credentials, proprietary docs). Apply retention, redaction, and access controls.
6. **Evaluation must include adversarial tests** (prompt injection, tool misuse, data exfil paths), not just quality metrics.
7. **Supply chain risk is shifting “up” to prompts and configs** (system prompts, tool schemas, memory stores) as well as models.
8. **Guardrails that only post-process text are insufficient** when the agent can call tools; enforce *pre-tool* policy.
9. **Human-in-the-loop needs to be real** (clear approvals, diff previews, scoped actions), not a checkbox.
10. **Incident response needs new artifacts**: prompt transcripts, tool call logs, retrieved docs, and policy decisions.

## Deep dive #1 — A read-only “agent safety model” that actually works
**Goal:** prevent the agent from doing anything you wouldn’t allow a low-privileged service account to do.

Key controls:
- **Capability allowlists** per tool (e.g., *read logs*, *query status*, *open ticket*), not generic “shell access.”
- **Resource scoping** (only these hosts, only this API, only these repos, only this directory).
- **Policy checks before tool execution** (deny-by-default for write actions).
- **Explainability + audit**: store structured logs of every tool call attempt and decision.

Practical pattern:
- Put a *policy gate* in front of all actions.
- Model sees *safe abstractions*, not raw credentials or raw network access.

## Deep dive #2 — Prompt injection: what defenders can do today
Threat model:
- Attacker controls text the model reads (web page, email, doc, ticket).
- That text tries to override instructions and/or exfiltrate secrets.

Defenses that work in practice:
- **Strict data vs instruction separation** (labels + render untrusted text as quoted data).
- **Never pass secrets into the same context** as untrusted content.
- **Minimize tool surface**: prefer narrow “query” tools over general-purpose tools.
- **Outbound egress controls** for tool calls (block unknown domains, file uploads).
- **Canary tokens / honey prompts** to detect exfil attempts in logs.

## Watch list (what to keep an eye on)
- “Agentic” workflows that combine browsing + file access + messaging.
- Memory features that persist user data across runs.
- Overuse of long-lived API tokens in automation.
- Shadow deployments: teams shipping LLM features without threat modeling.

## Key security recommendations (actionable)
1. **Implement deny-by-default tool policies** and require explicit allowlists per workflow.
2. **Create a dedicated secrets boundary** (never in prompt context; fetch just-in-time; redact logs).
3. **Add adversarial evals to CI**: prompt injection corpora + tool misuse tests.
4. **Adopt structured logging** for agent runs (inputs, retrieved docs, tool calls, outputs) with retention limits.
5. **Separate identities**: service accounts for read-only monitoring; approvals for any write actions.

## TODO (sources)
- Add citations for each section (vendor writeups, academic papers, and incident reports).
- Swap “no-live-sources” for sourced links once web_search is configured.

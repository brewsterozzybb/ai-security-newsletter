# AI Security Newsletter — 2026-02-01

> **no-live-sources**: Web search is unavailable in this environment (missing Brave API key), so this issue is a **high-quality, practitioner-focused brief from general knowledge** rather than a link-driven roundup of the last 7–14 days.
>
> **TODO (sources)** at the end lists the exact items to look up and link once live sources are available.

## TL;DR (10 bullets)

1. **Agentic LLMs are becoming “integration-heavy,” not “model-heavy.”** The dominant risk is shifting from model weights to the **tool layer** (connectors, automations, RPA, MCP-style tool buses): auth scopes, input sanitization, and auditability.
2. **Prompt injection is now a *cross-boundary* attack.** Treat untrusted content (email, tickets, docs, web pages, code comments) as **active adversarial input** capable of steering tools and data flows.
3. **RAG is an exfiltration surface.** Retrieval pipelines can leak via: (a) over-broad retrieval, (b) hidden instructions in retrieved text, (c) “answer formatting” coercion, and (d) embeddings-based inference about sensitive corpora.
4. **Tool output is as dangerous as tool input.** Attacks increasingly target *model-to-tool-to-model loops* (e.g., tool returns malicious text → model treats as instruction → escalates actions).
5. **Model “guardrails” are not a control plane.** If a system can perform actions, you need **policy enforcement outside the model** (allowlists, transactional approvals, rate limits, and sandboxing).
6. **Secret handling is the #1 fragility point.** Agents that “helpfully” copy/paste tokens, cookies, or API keys into logs, chats, or prompts create persistent compromise.
7. **Supply chain risk is expanding to ML artifacts.** Datasets, embedding indexes, fine-tunes, evaluation harnesses, and prompt libraries are now part of the **trusted computing base**—and often lack signing, provenance, and change control.
8. **Model extraction and membership inference remain practical threats** in some settings: high query budgets, weak rate limiting, and verbose error channels increase risk.
9. **Evaluation needs to include *security properties*, not just accuracy.** Add red-team suites for prompt injection, tool misuse, data exfil, and policy bypass; measure *action safety*.
10. **Incident response for AI systems must be pre-designed.** Without replayable traces and deterministic-ish environments, you’ll fail to root-cause tool misuse, data leakage, and hallucinated-but-acted outputs.

---

## Deeper Dive #1 — Prompt Injection in Agentic Tooling (Threat Model + Controls)

### Why this matters now
As organizations deploy “AI copilots” that can **read** enterprise content and **act** via tools (tickets, code, email, cloud, identity), prompt injection becomes closer to **classic command injection** than “model misbehavior.” The attacker’s objective is not to make the model *say* something; it’s to make the system **do** something.

### Common attack paths
- **Indirect prompt injection via documents**: malicious instructions embedded in PDFs, webpages, docs, or knowledge base pages that the agent retrieves and summarizes.
- **Toolchain coercion**: the agent is induced to call a tool with attacker-chosen parameters (e.g., “search for API keys,” “export customer list,” “create OAuth token”).
- **Instruction smuggling**: content formatted as quotes, logs, JSON, code blocks, HTML comments, or base64 to trick “instruction detectors.”
- **Cross-step persistence**: injected instructions survive through tool outputs, cached context, or memory layers.

### Defensive posture (practical)
1. **Untrusted content labeling**
   - Tag inputs by provenance (user, web, internal wiki, email) and enforce different policies.
   - Require the model to treat retrieved content as *data*, not *instructions*.
2. **Tool access control outside the model**
   - Implement **capability-based tooling**: each agent run gets a scoped, time-bound token per tool.
   - Enforce allowlists/denylists and parameter constraints (e.g., cannot access “/admin/*”, cannot export >N rows).
3. **Two-phase execution (“plan then act” with a gate)**
   - Force a structured plan output.
   - Run a policy engine to approve/deny each tool call based on context, user role, and data classification.
   - Add human-in-the-loop for high-risk actions.
4. **Prompt injection regression tests**
   - Maintain an internal corpus of adversarial snippets (HTML hidden text, doc footers, code comments).
   - Run these through your RAG + agent pipeline; fail builds when tool calls deviate.
5. **Traceability**
   - Log: input hashes, retrieved doc IDs, tool call parameters/results, and final outputs.
   - Ensure logs exclude secrets (token redaction).

### Key takeaway
If your agent can call tools, **prompt injection is an authorization problem**. Treat the model as an untrusted component that proposes actions, not as the enforcer.

---

## Deeper Dive #2 — RAG Security: Retrieval, Embeddings, and Data Leakage

### The failure mode
RAG systems often assume: “we only retrieve what’s relevant.” In practice:
- Retrieval can be **over-broad**, pulling adjacent sensitive content.
- “Relevant” can be adversarially manipulated (keyword stuffing, embedding collisions, doc poisoning).
- The model can be coerced to reveal raw snippets, internal identifiers, or summarize sensitive material beyond the user’s authorization.

### RAG-specific threats
- **Index poisoning**: attacker inserts or modifies documents to influence retrieval and downstream actions.
- **Policy bypass via retrieval**: user lacks access to data, but the RAG layer retrieves it anyway due to missing ACL integration.
- **Embedding leakage**: embeddings can encode properties of sensitive data; similarity queries can be used to infer membership in some regimes.
- **Data exfil via “formatting demands”**: “Print the exact retrieved passages,” “show the raw context,” “dump the top 50 chunks.”

### Controls that actually work
1. **Authorization at retrieval time (not response time)**
   - Integrate document-level ACLs into the retriever.
   - Filter candidate chunks by user identity and data classification.
2. **Context minimization**
   - Cap chunk count and length.
   - Use *purpose-specific* retrieval (e.g., “answer” vs “cite”) with different limits.
3. **Sensitive-data detectors before indexing and before responding**
   - Scan docs for secrets/PII; quarantine or mask before indexing.
   - Post-process responses for secrets, IDs, and regulated fields.
4. **Poisoning resistance**
   - Require provenance and change control for indexed sources.
   - Score and down-rank low-trust sources; separate indexes by trust tier.
5. **Red-team the retriever**
   - Tests for: “retrieve admin docs as a low-priv user,” “retrieve credentials with vague prompts,” “retrieve hidden instructions.”

### Key takeaway
RAG security is identity + provenance + minimization. If retrieval ignores auth and trust, your chatbot becomes a **search-powered data leak**.

---

## Watch List (what to monitor this week)

- **MCP / tool-bus ecosystems**: emerging “standard tool protocols” increase integration speed but also create a common exploitation layer.
- **Enterprise copilots in email/CRM/ticketing**: highest likelihood of indirect injection via untrusted inbound content.
- **Model & prompt supply chain**: third-party prompt packs, eval harnesses, and “agent templates” used in production without review.
- **AI-assisted code changes**: risk of quietly introducing insecure patterns (auth bypass, unsafe deserialization) in refactors.
- **Cloud IAM + agents**: accidental privilege creep (agents accumulating roles/scopes over time).

---

## Key Security Recommendations (actionable)

1. **Make tool calls non-default**: require explicit justification + policy checks for every tool invocation.
2. **Scope everything**: per-run, per-tool, short-lived credentials; no long-lived tokens in prompts or memory.
3. **Adopt a “content firewall”**: classify inputs (trusted/untrusted), strip active content, and neutralize instruction-like patterns in retrieved text.
4. **Implement approval workflows** for destructive or high-impact actions (emailing externally, deleting, exporting, changing IAM).
5. **Log for forensics**: capture structured traces of retrieval + tool calls; redact secrets; store tamper-evident logs.
6. **Continuously red-team** with injection + exfil suites integrated into CI.
7. **Treat AI artifacts as software artifacts**: code review, signing, provenance, SBOM-like tracking for datasets/prompts/fine-tunes.

---

## TODO (Sources to add once live search works)

When web search is available, add links for items from the last 7–14 days and pin exact citations for:

- Recent **prompt injection / indirect injection** research and real-world incident writeups.
- Any major **agent framework** security advisories (tool call escaping, sandbox bypass, data exfil).
- Updates on **MCP / tool protocol** security guidance (authn/authz, sandboxing, prompt boundaries).
- New or updated **NIST / ISO / OWASP** guidance relevant to AI application security.
- Notable **model supply chain** incidents: dataset poisoning, malicious adapters, compromised model hubs.

---

*Prepared for Jason — focus: practical controls for enterprise AI deployments.*

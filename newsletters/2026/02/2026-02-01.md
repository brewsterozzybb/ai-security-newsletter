# AI Security Brief — 2026-02-01

**Status:** sourced via Exa MCP web search (links included).

## Sources (selected)


## 10 things to know (practitioner bullets)

1. **Prompt injection is still the top practical risk** for LLM apps ingesting untrusted content; treat retrieved text as hostile and separate instructions from data.
2. **Tool/agent permissions define your blast radius.** Over-privileged tools + weak scoping is the fastest path to real damage.
3. **Agentic cyber capability is accelerating.** Treat autonomous workflows as near-term reality, especially when models can operate with standard tools.
4. **Retrieval ≠ trust.** RAG can increase confident errors unless you enforce provenance and constraints.
5. **Context is a data-leak boundary.** Avoid putting secrets in prompts; segment sensitive data and minimize what’s exposed.
6. **Logs become a shadow data lake.** Apply retention, redaction, and access controls to prompt/tool logs.
7. **Guardrails must be pre-tool, not just post-text.** Enforce policy before executing actions.
8. **Evaluation needs adversarial coverage** (prompt injection, tool misuse, data exfil paths), not only quality metrics.
9. **Human-in-the-loop must be real**: diffs, previews, scoped approvals, and clear rollback.
10. **Incident response must capture new artifacts**: prompts, retrieved docs, tool calls, and policy decisions.

## Deep dive #1 — A practical read-only safety model for agents

**Goal:** make the agent as safe as a read-only service account.

Controls that hold up in practice:
- **Capability allowlists** per workflow (e.g., *list devices*, *fetch status*, *read ticket*)
- **Resource scoping** (only these hosts/APIs/repos; no wildcard egress)
- **Policy gate before tool execution** (deny-by-default for writes)
- **Auditable structured logs** for every tool call attempt and decision

## Deep dive #2 — Prompt injection defenses that work

Threat model: attacker controls text the model reads (web page/email/doc/ticket) and tries to override instructions or exfiltrate data.

Defenses:
- **Hard-separate data vs instructions** (render untrusted text as quoted/attached data)
- **Never co-locate secrets with untrusted content** in the same context
- **Minimize tool surface** (prefer narrow query tools)
- **Outbound egress controls** (domain allowlists, upload blocks)
- **Canary tokens / honey prompts** to detect exfil attempts

## Watch list

- Agents that combine browsing + file access + messaging
- Memory features that persist data across runs
- Long-lived tokens in automation
- Shadow deployments without threat modeling

## Key security recommendations (actionable)

1. **Deny-by-default tool policies**, explicit allowlists per workflow.
2. **Secrets boundary**: never in prompt context; redact logs.
3. **Adversarial evals in CI** (prompt injection + tool misuse).
4. **Structured logging with retention** for agent runs.
5. **Separate identities** and require approvals for writes.


# ğŸ›¡ï¸ AI Security Weekly â€” Issue #7
**Wednesday, February 18, 2026**

> *Covering the intersection of AI systems and security: vulnerabilities, tooling, research, and the arms race nobody asked for.*

---

## ğŸ”¥ This Week's Top Story

### Chainlit Hit With Two High-Severity CVEs â€” Patch Now

AI chat framework Chainlit (<2.9.4) has two nasty flaws in its `/project/element` update flow:

- **CVE-2026-22218** (CVSS 7.1â€“7.7): Arbitrary file read â€” authenticated attackers can read any file the Chainlit process can touch. Think `.env`, credentials, private keys.
- **CVE-2026-22219**: SSRF â€” same attack surface, but now pointed at your internal network and cloud metadata services (hello, AWS IMDSv1).

**Fix:** Upgrade to Chainlit â‰¥ 2.9.4 immediately if you're running any customer-facing deployments.

ğŸ”— [Infosecurity Magazine coverage](https://www.infosecurity-magazine.com/news/chainlit-security-flaws-ai-apps) | [GitLab Advisory (CVE-2026-22219)](https://advisories.gitlab.com/pkg/pypi/chainlit/CVE-2026-22219/)

---

## ğŸ§¨ Vulnerability Roundup

### n8n: 8 New CVEs, Critical Sandbox Escapes
n8n â€” the AI workflow orchestration platform practically everyone is stitching into their agentic stacks â€” picked up **8 new CVEs** this month (CVE-2026-1470, -25049, -25052, -25053, -25055, -25056, -25115, -0863). Several are critical and involve **sandbox escape and code execution**. If you have n8n exposed anywhere, treat it like a fresh breach: patch, audit, rotate secrets.

ğŸ”— [Geordie.ai: Technical Advisory](https://www.geordie.ai/resources/technical-advisory-eight-new-n8n-cves-since-january---updated-remediation-guidance)

### Salesforce AI + LangChain: RCE in Model Loading
- **CVE-2026-22584**: RCE when loading malicious model files in Salesforce AI research libraries. The "open a model, own the host" attack class is alive and well.
- **LangChain-Core**: Recently patched a critical vuln that allowed AI agent **secret compromise**. If you haven't updated your LangChain stack, do it before deploying more agents.

ğŸ”— [Unit42: RCE in AI Python Libraries](https://unit42.paloaltonetworks.com/rce-vulnerabilities-in-ai-python-libraries/) | [SC World: LangChain Advisory](https://www.scworld.com/brief/ai-agent-secret-compromise-possible-with-critical-langchain-core-vulnerability)

---

## ğŸ¯ Attacks & Research

### MCP Is Becoming a Serious Attack Surface
Two recent CVEs crystallize the problem:
- **CurXecute (CVE-2025-54135)**: RCE via MCP config manipulation in Cursor IDE.
- **MCPoison (CVE-2025-54136)**: Persistent, team-wide compromise via shared MCP repository configs â€” one poisoned config file hits your entire engineering org.

Meanwhile, a new arxiv paper (**[2602.11327](https://arxiv.org/abs/2602.11327)**) provides the first systematic threat model for AI agent communication protocols â€” MCP, A2A, Agora, and ANP. Key finding: **mandatory validation/attestation for executable components is largely absent**, and "wrong-provider tool execution" in multi-server setups is a documented, quantified risk. Required reading if you're building anything agentic.

### Zero-Click Data Exfiltration Is Getting Scary Good
From the LockLLM 2026 attack technique library:
- **EchoLeak (CVE-2025-32711)**: Steal data from Microsoft 365 Copilot via malicious emails encoding exfil in image URLs. No clicks required.
- **CamoLeak (CVE-2025-59145)**: GitHub Copilot exfil via image proxies â€” CVSS **9.6**.
- **GeminiJack**: Hidden instructions in Google Docs compromise Gemini Enterprise silently.

The pattern: AI assistants with access to your files + email + calendar are one malicious document away from exfiltrating everything. Defense today is a mix of DLP and praying.

### Claude Desktop Extensions: 10K+ Users Exposed to RCE
A vulnerability in Claude Desktop Extensions exposed over 10,000 users to remote code execution. Prompt injection â†’ RCE isn't a theoretical path anymore.

ğŸ”— [LayerX Security writeup](https://layerxsecurity.com/blog/claude-desktop-extensions-exposes-over-10000-users-to-remote-code-execution-vulnerability)

### Schneier: LLMs Are Getting Good at Finding Zero-Days, Fast
Bruce Schneier weighs in on the accelerating ability of LLMs to discover and exploit zero-days â€” not just assist researchers, but operate autonomously. The arms race implications are hard to overstate.

ğŸ”— [Schneier on Security](https://schneier.com/blog/archives/2026/02/llms-are-getting-a-lot-better-and-faster-at-finding-and-exploiting-zero-days.html)

---

## ğŸ› ï¸ Tools & Frameworks

### SENTINEL â€” Full-Stack AI Security Platform
[github.com/DmitrL-dev/AISecurity](https://github.com/DmitrL-dev/AISecurity)

116K+ lines of code covering both offense and defense for LLMs and agentic systems. Notable components:
- **SHIELD**: <1ms input/output protection layer
- **STRIKE**: 39,000+ offensive payloads for red teaming
- **Micro-Model Swarm**: ML jailbreak detection (F1=0.997)
- **SuperClaudeShield**: Protection wrapper specifically for Claude Code and Gemini Code
- **Academy**: 159 lessons, 8 labs, full OWASP LLM Top 10 + Agentic AI Top 10 coverage

```bash
pip install sentinel-llm-security
```

### Grok-Powered IR Playbook Generator
Describe an incident in plain text, get a full IR playbook in ~15 seconds â€” executive summary, MITRE ATT&CK mapping, containment/eradication steps, and an interactive Mermaid flowchart. Built on Streamlit + Grok 4 API. Free and open-source.

**Real talk:** "Hours â†’ seconds" is genuinely impressive. The bottleneck now is *decision execution*, not playbook creation. SOCs without SOAR platforms, this one's for you.

ğŸ”— [GitHub: JunkDrawer/Automated IR Playbook](https://github.com/rod-trent/JunkDrawer/tree/main/Automated%20Incident%20Response%20Playbook%20Generator)

---

## ğŸ“– Long Reads Worth Your Time

- **[Trusting Claude With a Knife](https://johnstawinski.com/2026/02/05/trusting-claude-with-a-knife-unauthorized-prompt-injection-to-rce-in-anthropics-claude-code-action)** â€” Unauthorized prompt injection â†’ RCE in Anthropic's Claude Code Action. Detailed technical breakdown.
- **[AWS Compromised by AI Agents in Minutes](https://vectra.ai/blog/aws-compromised-by-ai-agents-in-minutes)** â€” Vectra AI demonstrates how fast AI-assisted cloud compromise can move. Sobering.
- **[dreadnode: Worlds â€” Agentic Pentesting Simulation](https://dreadnode.io/blog/worlds-a-simulation-engine-for-agentic-pentesting)** â€” A simulation engine for agentic pentesting workflows. Worth watching.
- **[Teleport: Prevent Prompt Injection in AI Agents](https://goteleport.com/blog/prevent-prompt-injection-in-ai-agents)** â€” Practical defensive guide. One of the better how-to's this week.
- **[Martin Fowler: Agentic AI and Security](https://martinfowler.com/articles/agent)** â€” Architecture-level thinking on agentic security patterns.
- **[CSA: LPCI Vulnerability (Logic-Layer Prompt Control Injection)](https://cloudsecurityalliance.org/blog/2026/02/logic-layer-prompt-control-injection)** â€” A novel vuln class worth understanding early.

---

## ğŸ“Š Stat of the Week

> **100,000 prompts** â€” The number Google estimates attackers used attempting to clone Gemini via model extraction.
> Model extraction attacks are becoming brute-force operations. The barrier to stealing a fine-tuned model's behavior is dropping fast.

ğŸ”— [Google Cloud blog](https://cloud.google.com/blog/topics/threat-intelligence/google-says-attackers-used-100000-prompts-to-try-to-clone-ai-chatbot-gemini)

---

## ğŸ´ Red Team Corner

From the **LockLLM 2026** library â€” attack success rates that should unsettle you:

| Technique | Target | Success Rate |
|-----------|--------|-------------|
| X-Teaming (multi-turn adaptive) | GPT-5.2 / Gemini 3 Pro | 98% |
| Paper Summary Attack | Claude 3.5 / DeepSeek-R1 | 97%+ |
| Vector Embedding Poisoning | RAG systems (1M docs) | 90% (5 docs injected) |
| ArtPrompt (ASCII art injection) | Vision-Language Models | 76% |
| Direct Prompt Injection | Claude Opus 4.5 | ~5% âœ… |

Claude Opus 4.5 holds the line on direct injection best â€” but is still vulnerable to specific targeted hijacking (CVE-2025-54794). Defense-in-depth, always.

ğŸ”— [LockLLM: Full Attack Library](https://www.lockllm.com/blog/llm-attack-techniques-2026)

---

*Next issue: Thursday, February 26.*
*Tips, CVEs, or tools to feature? Drop them in the research vault.*

---
**Issue #7 | AI Security Weekly | February 2026**
